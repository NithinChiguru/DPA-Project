# DPA-Project

Abstract:
Handwritten letter recognition continues to pose significant challenges in computer vision, owing to the immense variability of human writing styles and the presence of noise in scanned or photographed documents. In this project, we employ a semi supervised convolutional neural network that leverages pseudo labeling to harness a large pool of unlabeled examples from the EMNIST Letters dataset. After training the model on a modest labeled subset for five epochs, we generate pseudo labels for high confidence unlabeled samples (confidence ≥ 0.90) and retrain for two additional epochs on the combined dataset. This approach yields a test accuracy of 98.32%, confirming that pseudo labeling effectively augments performance while reducing manual annotation effort. Key insights include the critical role of confidence thresholds in balancing label quality and quantity, the stabilizing effect of min–max normalization on gradient updates, and common error patterns such as C vs. G confusion. Looking ahead, integrating consistency-regularization techniques and extending support to cursive forms represent promising next steps.

Overview
Handwritten letters are at the heart of countless historical archives and everyday workflows, yet their free form nature makes them notoriously difficult for machines to interpret. In our project, we set out to build a system in R that reads and recognizes the 26 letters of the English alphabet with minimal labeled data. By combining a small set of human annotated examples with a much larger pool of unlabeled images, we aim to teach a convolutional neural network (CNN) to discern the subtle shapes and strokes that distinguish each character.
We begin by training the CNN on a carefully stratified subset of labeled images, allowing it to learn core features like edges and curves. Once this initial model is in place, we let it “guess” labels for the unlabeled images—but only keep those guesses when it is highly confident (at least 90%). These high-confidence predictions are then folded back into the training data, and the model is fine tuned with this expanded dataset. This pseudo-labeling strategy helps the network generalize better without requiring us to hand label every single image.
This document walks you through our entire process. (Abstract) presents our key findings and next steps; Section 1 (Overview) explains our goals and approach, with subsections 1.2 (Objectives) and 1.3 (Specific Questions) framing our targets and research inquiries; Section 2 (Literature Survey) Reviews prior work on handwriting recognition: rule-based, statistical, and deep-learning approaches, datasets, and evaluation metrics; Section 3 (Data Preparation & Cleansing) details how we collected, cleaned, and normalized over 300,000 letter images; Section 4 (Modeling) describes the CNN architecture and two-phase pseudo-label training cycle; and (Results & Analysis) reports the 98.32% test accuracy, precision/recall/F1 metrics, and confusion-matrix insights; Section 5 (Implementation & Deployment) covers our RStudio setup, R Markdown workflows, and reproducibility measures; Section 6,7 (Future Work & Conclusion) summarizes our takeaways and outlines next research directions; and finally, Sections 8,9 and 10 list our data sources, code repository, and full bibliography.

Objective
Our primary objectives are to develop an R based letter recognition pipeline that achieves at least 98% accuracy on the EMNIST Letters dataset, while minimizing the volume of manually labeled data. We aim to demonstrate that pseudo labeling—a process of adding only high confidence model predictions to training—can effectively boost performance. Additionally, we seek to provide a fully reproducible, end-to-end workflow from data ingestion through model evaluation, so others can easily build upon our results.

Specific questions
•	How accurately can a semi supervised CNN identify each of the 26 English letters when trained on only 20% labeled data?
•	What new capabilities and enhancements should future iterations of our semi supervised model explore to keep pace with evolving handwriting styles and applications?
•	Which data preprocessing techniques (e.g., thresholding, smoothing, normalization) contribute most to performance gains?
•	In what practical scenarios—such as archival digitization or real time form entry—can our recognition pipeline deliver the greatest impact?
